{
  "code": "def prepare_model_data(\n    raw_data: pd.DataFrame,\n    params: Dict,\n) -> Tuple[pd.DataFrame, str]:\n    \"\"\"Prepare data for model training with configurable target.\n    \n    Args:\n        raw_data: Raw segmentation data from parquet file\n        params: Parameters from conf/base/parameters.yml containing:\n            - feature_columns: List of feature column names to use\n            - target_column: Name of the target column to predict\n        \n    Returns:\n        Tuple of:\n        - Prepared dataframe with features and target\n        - Feature column names as comma-separated string\n    \"\"\"\n    logger.info(\"Starting data preparation for model training\")\n    \n    # Get parameters\n    feature_columns = params.get(\"feature_columns\", [\"age\", \"income\", \"credit_score\"])\n    target_column = params.get(\"target_column\")\n    reference_columns = params.get(\"reference_columns\", []) or []\n    if isinstance(reference_columns, str):  # allow comma-separated string\n        reference_columns = [c.strip() for c in reference_columns.split(\",\") if c.strip()]\n\n    # Optional explicit old model prediction column (e.g. prediction_B) passed via params\n    old_model_column = params.get(\"old_model_column\")\n    if old_model_column and old_model_column not in reference_columns:\n        reference_columns.append(old_model_column)\n\n    # Forced passthrough columns (kept if present, regardless of reference_columns) - no hardcoded defaults\n    forced_passthrough = params.get(\"forced_passthrough_columns\", []) or []\n    if isinstance(forced_passthrough, str):\n        forced_passthrough = [c.strip() for c in forced_passthrough.split(\",\") if c.strip()]\n    # Ensure old_model_column is also in forced passthrough list if provided\n    if old_model_column and old_model_column not in forced_passthrough:\n        forced_passthrough.append(old_model_column)\n    \n    if not target_column:\n        raise ValueError(\"target_column must be specified in parameters\")\n    \n    logger.info(f\"Using features: {feature_columns}\")\n    logger.info(f\"Using target: {target_column}\")\n    \n    # Validate that all required columns exist in the data\n    missing_features = [col for col in feature_columns if col not in raw_data.columns]\n    if missing_features:\n        raise ValueError(f\"Missing feature columns in data: {missing_features}\")\n    \n    if target_column not in raw_data.columns:\n        raise ValueError(f\"Target column '{target_column}' not found in data\")\n    \n    # Select features, target and any reference (passthrough) columns present\n    columns_to_keep = [*feature_columns, target_column]\n    existing_refs = [c for c in reference_columns if c in raw_data.columns and c not in columns_to_keep]\n    if existing_refs:\n        logger.info(f\"Including reference columns (passthrough, not for training): {existing_refs}\")\n        columns_to_keep.extend(existing_refs)\n    if old_model_column and old_model_column in existing_refs:\n        logger.info(f\"Old model column '{old_model_column}' retained for evaluation only\")\n\n    # Add forced passthrough columns if they exist in data\n    forced_present = [c for c in forced_passthrough if c in raw_data.columns and c not in columns_to_keep]\n    if forced_present:\n        logger.info(f\"Including forced passthrough columns (always kept, not for training): {forced_present}\")\n        columns_to_keep.extend(forced_present)\n    missing_refs = [c for c in reference_columns if c not in raw_data.columns]\n    if missing_refs:\n        logger.warning(f\"Reference columns not found and will be skipped: {missing_refs}\")\n    prepared_data = raw_data[columns_to_keep].copy()\n    \n    # Basic data quality checks\n    logger.info(f\"Original data shape: {raw_data.shape}\")\n    logger.info(f\"Prepared data shape: {prepared_data.shape}\")\n    \n    # Check for missing values\n    missing_counts = prepared_data.isna().sum()\n    if missing_counts.sum() > 0:\n        logger.warning(f\"Missing values found: {missing_counts[missing_counts > 0].to_dict()}\")\n    \n    # Remove rows with missing values if any\n    initial_rows = len(prepared_data)\n    prepared_data = prepared_data.dropna()\n    final_rows = len(prepared_data)\n    \n    if initial_rows != final_rows:\n        logger.info(f\"Removed {initial_rows - final_rows} rows with missing values\")\n    \n    # Basic statistics\n    logger.info(\"Feature statistics:\")\n    for col in feature_columns:\n        logger.info(f\"  {col}: mean={prepared_data[col].mean():.2f}, \"\n                   f\"std={prepared_data[col].std():.2f}, \"\n                   f\"min={prepared_data[col].min():.2f}, \"\n                   f\"max={prepared_data[col].max():.2f}\")\n    \n    logger.info(\"Target statistics:\")\n    logger.info(f\"  {target_column}: mean={prepared_data[target_column].mean():.2f}, \"\n               f\"std={prepared_data[target_column].std():.2f}, \"\n               f\"min={prepared_data[target_column].min()}, \"\n               f\"max={prepared_data[target_column].max()}\")\n    \n    logger.info(\"Data preparation completed successfully\")\n    \n    # Convert feature columns list to comma-separated string for text storage\n    feature_columns_str = \",\".join(feature_columns)\n    \n    return prepared_data, feature_columns_str\n",
  "filepath": "kedropipe/src/modelcreation/pipelines/data_preparation/nodes.py",
  "parameters": {
    "data_preparation": {
      "feature_columns": [
        "age",
        "income",
        "credit_score"
      ],
      "target_column": "target_B",
      "old_model_column": "prediction_B"
    }
  },
  "run_command": "kedro run --to-nodes='prepare_model_data_node'",
  "inputs": [
    "raw_segmentation_data",
    "params:data_preparation"
  ],
  "outputs": [
    "prepared_model_data",
    "feature_columns"
  ]
}