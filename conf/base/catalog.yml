# Here you can define all your datasets by using simple YAML syntax.
#
# Documentation for this file format can be found in "The Data Catalog"
# Link: https://docs.kedro.org/en/stable/data/data_catalog.html

# Raw data
raw_segmentation_data:
  type: pandas.ParquetDataset
  filepath: data/raw/segmentation_data.parquet

# Data preparation outputs  
prepared_model_data:
  type: pandas.ParquetDataset
  filepath: data/processed/prepared_model_data.parquet

# Store feature columns as CSV for simplicity
feature_columns:
  type: text.TextDataset
  filepath: data/processed/feature_columns.txt

# Model training datasets
X_train:
  type: pandas.ParquetDataset
  filepath: data/processed/X_train.parquet

X_test:
  type: pandas.ParquetDataset
  filepath: data/processed/X_test.parquet

y_train:
  type: text.TextDataset  # Store as CSV-like text
  filepath: data/processed/y_train.txt

y_test:
  type: text.TextDataset  # Store as CSV-like text
  filepath: data/processed/y_test.txt

# Model outputs
trained_model:
  type: pickle.PickleDataset  
  filepath: data/models/trained_catboost_model.pkl

model_metrics:
  type: text.TextDataset  # Will store as text temporarily
  filepath: data/models/model_metrics.txt

# Model validation outputs
model_predictions:
  type: pandas.ParquetDataset
  filepath: data/processed/model_predictions.parquet

validation_dataset:
  type: pandas.ParquetDataset
  filepath: data/processed/validation_dataset.parquet

validation_metrics:
  type: pandas.ParquetDataset
  filepath: data/processed/validation_metrics.parquet

segmented_metrics:
  type: pickle.PickleDataset
  filepath: data/processed/segmented_metrics.pkl

model_comparison_results:
  type: json.JSONDataset
  filepath: data/processed/model_comparison_results.json

validation_report_paths:
  type: json.JSONDataset
  filepath: data/reporting/validation_report_paths.json
